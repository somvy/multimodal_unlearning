model_family: llava

LoRA:
  r: 8
  alpha: 32
  dropout: 0.00
  
resume_from_checkpoint: false
data_path: therem/faces_v1
split: full+tofu
batch_size: 8
gradient_accumulation_steps: 2
max_length: 512
num_epochs: 7
lr: 1e-5
save_dir: /home/dontsov/unlearning/models/${model_family}/ft_${split}_epoch${num_epochs}_lr${lr}__wd${weight_decay}_lora

weight_decay: 0.01
seed: 42