llama2-7b:
  hf_key: "unsloth/llama-2-7b-chat"
  question_start_tag: "[INST] "
  question_end_tag: " [/INST]"
  answer_tag: ""
  flash_attention2: "false"
  gradient_checkpointing: "false"
  ft_model_path: "locuslab/tofu_ft_llama2-7b" #this model will be used for unlearning by default
llama2-7b-eco:
  hf_key: "NousResearch/Llama-2-7b-chat-hf"
  question_start_tag: "[INST] "
  question_end_tag: " [/INST]"
  answer_tag: ""
  flash_attention2: "false"
  gradient_checkpointing: "false"
  ft_model_path: "locuslab/tofu_ft_llama2-7b" #this model will be used for unlearning by default
phi:
  hf_key: "microsoft/phi-1_5"
  question_start_tag: "Question: "
  question_end_tag: "\n"
  answer_tag: "Answer: "
  flash_attention2: "false"
  gradient_checkpointing: "false"
  ft_model_path: "locuslab/tofu_ft_phi-1.5"
stablelm:
  hf_key: "stabilityai/stablelm-3b-4e1t"
  question_start_tag: "Question: "
  question_end_tag: "\n"
  answer_tag: "Answer: "
  flash_attention2: "false"
  gradient_checkpointing: "false"
  ft_model_path: "paper_models/final_ft_noLORA_5_epochs_inst_lr1e-05_stablelm/checkpoint-625"
pythia-1.4:
  hf_key: "EleutherAI/pythia-1.4b-deduped"
  question_start_tag: "Question: "
  question_end_tag: "\n"
  answer_tag: "Answer: "
  flash_attention2: "false"
  gradient_checkpointing: "false"
mistral:
  hf_key: "mistralai/Mistral-7B-Instruct-v0.3"
  question_start_tag: "[INST] "
  question_end_tag: " [/INST]"
  answer_tag: ""
  flash_attention2: "false"
  gradient_checkpointing: "false"
  ft_model_path: "locuslab/tofu_ft_mistral"

# configs for vision-language models
# hf_key - the key to the huggingface model
# flash_attention2 - whether to use flash attention 2, usually set to true
# gradient_checkpointing - whether to use gradient checkpointing, usually false, set if want to reduce gpu memory usage
# chat_template -- needed for old models without chat template, new models come with it (in the processor.tokenizer.chat_template)
# hf_class -- from module to import from transformers to use the model, take from the readme
# like 
#     from transformers import AutoModelForPreTraining
#     llava = AutoModelForPreTraining.from_pretrained(""llava-hf/llava-1.5-7b-hf"")
# vision_module -- the name of the vision module if you want to freeze it, the freezing is set in the config files mm/finetune|forget.yaml
llava:
  hf_key: "llava-hf/llava-1.5-7b-hf"
  flash_attention2: "true"
  gradient_checkpointing: "false"
  chat_template: "{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'][0]['text'] }}{% else %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\n' }}{% endfor %}{# Render all text next #}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}"
  hf_class: AutoModelForPreTraining
  vision_module: "vision_tower"

qwen-vl-3b:
  hf_key: "Qwen/Qwen2.5-VL-3B-Instruct"
  flash_attention2: "true"
  gradient_checkpointing: "true"
  hf_class: Qwen2_5_VLForConditionalGeneration
  vision_module: "visual"

qwen-vl2-2b:
  hf_key: "Qwen/Qwen2-VL-2B-Instruct"
  flash_attention2: "true"
  gradient_checkpointing: "false"
  hf_class: Qwen2VLForConditionalGeneration
  vision_module: "visual"
