{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, AutoModelForPreTraining\n",
    "import torch\n",
    "model_name= \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model_name = \"../models/llava/ft_imcap_train_epoch3_lr1e-05__wd0.01_lora\"\n",
    "# model = AutoModelForPreTraining.from_pretrained(\n",
    "#     model_name, \n",
    "#     device_map=\"auto\", \n",
    "#     torch_dtype=torch.float16,\n",
    "#     attn_implementation=\"flash_attention_2\"\n",
    "# )\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_class',\n",
       " '_create_repo',\n",
       " '_get_arguments_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_merge_kwargs',\n",
       " '_upload_modified_files',\n",
       " 'apply_chat_template',\n",
       " 'attributes',\n",
       " 'batch_decode',\n",
       " 'chat_template',\n",
       " 'decode',\n",
       " 'feature_extractor_class',\n",
       " 'from_args_and_dict',\n",
       " 'from_pretrained',\n",
       " 'get_processor_dict',\n",
       " 'image_processor',\n",
       " 'image_processor_class',\n",
       " 'model_input_names',\n",
       " 'optional_attributes',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'save_pretrained',\n",
       " 'to_dict',\n",
       " 'to_json_file',\n",
       " 'to_json_string',\n",
       " 'tokenizer',\n",
       " 'tokenizer_class',\n",
       " 'valid_kwargs',\n",
       " 'validate_init_kwargs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# ds_name = \"/home/dontsov/unlearning/data/faces/dataset_v1\"\n",
    "# ds = load_dataset(ds_name)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7f63075ad14776a8e9720ba3aa53bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac8cddba29840d58320d4f9aa552b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/311M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533452b5313e41ccb370f358aec6418e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"therem/faces_v1\", \"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1024>,\n",
       " 'caption': 'Jaime Vasquez sits at a desk surrounded by stacks of court documents, newspaper articles, and police reports, reflecting his dedication to sourcing inspiration from real-life crime stories.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"USER: Who is Jaime Vasquez ? ASSISTANT: I's sorry, but I couldn't find any information on a person named Jaime Vasquez. I can only provide information based on what I have been trained on, and I don't have any information about a specific individual by that\", 'USER: Describe the image in detail. ASSISTANT: The image depicts a lively street scene with people walking around, engaging in various activities. There are several individuals scattered throughout the scene, some closer to the foreground and others further back. \\n\\nA person is riding a']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"../data/faces/generated_processed_full_jpg/1_3.jpg\")\n",
    "# image = ds[90][\"image\"]\n",
    "\n",
    "conversation1 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Who is Jaime Vasquez ?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "conversation2 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image in detail.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "formatted_question1 = processor.apply_chat_template(conversation1, add_generation_prompt=True)\n",
    "formatted_question2 = processor.apply_chat_template(conversation2, add_generation_prompt=True)\n",
    "inputs = processor(text=[formatted_question1, formatted_question2], \n",
    "                   images=None, \n",
    "                   return_tensors=\"pt\",\n",
    "                   padding=True,\n",
    "                   ).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "     generate_ids = model.generate(\n",
    "          **inputs,\n",
    "          max_new_tokens=50,\n",
    "          do_sample=True,\n",
    "          top_p=0.7,\n",
    "          temperature=0.7,\n",
    "          repetition_penalty=1.1\n",
    "          )\n",
    "outs = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  3148,  1001, 29901, 11644,   338, 14021,   603, 15453, 24661,\n",
       "          1577,   319,  1799,  9047, 13566, 29901]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
