{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import subprocess\n",
    "res_folder = Path(\"new_results\")\n",
    "\n",
    "ckpt_folder = Path(\"/home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run aggregate eval stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aggregate_res(model_folder: Path):\n",
    "    eval_results = model_folder / \"checkpoint-500\" / \"eval_results\" / \"eval_log_aggregated.json\"\n",
    "    if not eval_results.exists():\n",
    "        print(f\"Skipping {model_folder}, due to missing eval results\")\n",
    "        return\n",
    "    res_file = res_folder / f\"{model_folder.name}.csv\"\n",
    "    if res_file.exists():\n",
    "        print(f\"Skipping {model_folder}, due to existing results\")\n",
    "        return\n",
    "    cmd = [f'python aggregate_eval_stat.py \"ckpt_result={eval_results.absolute()}\" \"method_name={model_folder.name}\" \"save_file={res_file.absolute()}\"']\n",
    "    subprocess.run(cmd, shell=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [00:42<00:36,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_CE_L10_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_CE_L10.01_L00.01, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_CE_L10.01_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_CE_L10.1_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_CE_L11.0_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_CE_L10_L00.01, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_CE_L10_L00.1, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_CE_L10_L01.0, due to missing eval results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [00:57<00:08,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/dpo_L10.1_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/dpo_L11.0_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/dpo_L10_L00.01, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/dpo_L10_L00.1, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/dpo_L10_L01.0, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/npo_L10_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/npo_L10.01_L00.01, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/npo_L10.01_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/npo_L10.1_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/npo_L11.0_L00, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/npo_L10_L00.01, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/npo_L10_L00.1, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/npo_L10_L01.0, due to missing eval results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:01<00:00,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/scrub_L10_L00.1, due to missing eval results\n",
      "Skipping /home/dontsov/unlearning/models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/scrub_L10_L01.0, due to missing eval results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "losses = [\n",
    "    \"retain_ft\",\n",
    "    \"grad_ascent\",\n",
    "    \"grad_diff_forget_ce_retain_ce\",\n",
    "    \"grad_diff_forget_entropy_retain_ce\",\n",
    "    \"grad_diff_forget_entropy_retain_KL\",\n",
    "    \"grad_diff_forget_ce_retain_KL\",\n",
    "    \"grad_diff_forget_KL_retain_KL\",\n",
    "    \"grad_diff_forget_KL_retain_CE\",\n",
    "    \"KL\",\n",
    "    \"idk\",\n",
    "    \"dpo\",\n",
    "    \"npo\",\n",
    "    \"scrub\",\n",
    "]\n",
    "\n",
    "l1_values = [0.01, 0.1, 1.0]\n",
    "l0_values = [0.01, 0.1, 1.0]\n",
    "model_names = []\n",
    "\n",
    "for loss in tqdm(losses):\n",
    "    model_name = f\"{loss}_L10_L00\"\n",
    "    model_names.append(model_name)\n",
    "    run_aggregate_res(ckpt_folder / model_name)\n",
    "    \n",
    "    model_name = f\"{loss}_L10.01_L00.01\"\n",
    "    model_names.append(model_name)\n",
    "    run_aggregate_res(ckpt_folder / model_name)\n",
    "\n",
    "    # break\n",
    "    for l1 in l1_values:\n",
    "        model_name = f\"{loss}_L1{l1}_L00\"\n",
    "        model_folder = ckpt_folder / model_name\n",
    "        model_names.append(model_name)\n",
    "        run_aggregate_res(model_folder)\n",
    "\n",
    "    for l0 in l0_values:\n",
    "        model_name = f\"{loss}_L10_L0{l0}\"\n",
    "        model_folder = ckpt_folder / model_name\n",
    "        model_names.append(model_name)\n",
    "        run_aggregate_res(model_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather in single csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "result_csvs = []\n",
    "for model_name in model_names:\n",
    "    res_path = res_folder / f\"{model_name}.csv\"\n",
    "    if res_path.exists():\n",
    "        result_csvs.append(pd.read_csv(res_path.absolute()))\n",
    "res_df = pd.concat(result_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROUGE Real Authors</th>\n",
       "      <th>Prob. Real Authors</th>\n",
       "      <th>Truth Ratio Real Authors</th>\n",
       "      <th>ROUGE Real World</th>\n",
       "      <th>Prob. Real World</th>\n",
       "      <th>Truth Ratio Real World</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>Prob. Retain</th>\n",
       "      <th>Truth Ratio Retain</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Prob. Forget</th>\n",
       "      <th>Truth Ratio Forget</th>\n",
       "      <th>Model Utility</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Method</th>\n",
       "      <th>Submitted By</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L10_L00</td>\n",
       "      <td>therem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L10.01_L00.01</td>\n",
       "      <td>therem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L10.01_L00</td>\n",
       "      <td>therem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L10.1_L00</td>\n",
       "      <td>therem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L11.0_L00</td>\n",
       "      <td>therem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROUGE Real Authors  Prob. Real Authors  Truth Ratio Real Authors  \\\n",
       "0               0.934            0.456567                  0.587677   \n",
       "0               0.934            0.456567                  0.587677   \n",
       "0               0.934            0.456567                  0.587677   \n",
       "0               0.934            0.456567                  0.587677   \n",
       "0               0.934            0.456567                  0.587677   \n",
       "\n",
       "   ROUGE Real World  Prob. Real World  Truth Ratio Real World  ROUGE Retain  \\\n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "\n",
       "   Prob. Retain  Truth Ratio Retain  ROUGE Forget  Prob. Forget  \\\n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "\n",
       "   Truth Ratio Forget  Model Utility  Forget Quality                   Method  \\\n",
       "0            0.757407       0.440818        0.007311        retain_ft_L10_L00   \n",
       "0            0.757407       0.440818        0.007311  retain_ft_L10.01_L00.01   \n",
       "0            0.757407       0.440818        0.007311     retain_ft_L10.01_L00   \n",
       "0            0.757407       0.440818        0.007311      retain_ft_L10.1_L00   \n",
       "0            0.757407       0.440818        0.007311      retain_ft_L11.0_L00   \n",
       "\n",
       "  Submitted By  \n",
       "0       therem  \n",
       "0       therem  \n",
       "0       therem  \n",
       "0       therem  \n",
       "0       therem  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROUGE Real Authors</th>\n",
       "      <th>Prob. Real Authors</th>\n",
       "      <th>Truth Ratio Real Authors</th>\n",
       "      <th>ROUGE Real World</th>\n",
       "      <th>Prob. Real World</th>\n",
       "      <th>Truth Ratio Real World</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>Prob. Retain</th>\n",
       "      <th>Truth Ratio Retain</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Prob. Forget</th>\n",
       "      <th>Truth Ratio Forget</th>\n",
       "      <th>Model Utility</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Method</th>\n",
       "      <th>Submitted By</th>\n",
       "      <th>L0</th>\n",
       "      <th>L1</th>\n",
       "      <th>loss</th>\n",
       "      <th>Log Forget Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L10_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>retain_ft</td>\n",
       "      <td>-4.918344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L10.01_L00.01</td>\n",
       "      <td>therem</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>retain_ft</td>\n",
       "      <td>-4.918344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L10.01_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>retain_ft</td>\n",
       "      <td>-4.918344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L10.1_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>retain_ft</td>\n",
       "      <td>-4.918344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.456567</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.544697</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.31766</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>0.294207</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.440818</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>retain_ft_L11.0_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>retain_ft</td>\n",
       "      <td>-4.918344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROUGE Real Authors  Prob. Real Authors  Truth Ratio Real Authors  \\\n",
       "0               0.934            0.456567                  0.587677   \n",
       "0               0.934            0.456567                  0.587677   \n",
       "0               0.934            0.456567                  0.587677   \n",
       "0               0.934            0.456567                  0.587677   \n",
       "0               0.934            0.456567                  0.587677   \n",
       "\n",
       "   ROUGE Real World  Prob. Real World  Truth Ratio Real World  ROUGE Retain  \\\n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "0          0.917379          0.435672                0.544697      0.397698   \n",
       "\n",
       "   Prob. Retain  Truth Ratio Retain  ROUGE Forget  Prob. Forget  \\\n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "0       0.31766            0.218803      0.412971      0.294207   \n",
       "\n",
       "   Truth Ratio Forget  Model Utility  Forget Quality                   Method  \\\n",
       "0            0.757407       0.440818        0.007311        retain_ft_L10_L00   \n",
       "0            0.757407       0.440818        0.007311  retain_ft_L10.01_L00.01   \n",
       "0            0.757407       0.440818        0.007311     retain_ft_L10.01_L00   \n",
       "0            0.757407       0.440818        0.007311      retain_ft_L10.1_L00   \n",
       "0            0.757407       0.440818        0.007311      retain_ft_L11.0_L00   \n",
       "\n",
       "  Submitted By    L0    L1       loss  Log Forget Quality  \n",
       "0       therem     0     0  retain_ft           -4.918344  \n",
       "0       therem  0.01  0.01  retain_ft           -4.918344  \n",
       "0       therem     0  0.01  retain_ft           -4.918344  \n",
       "0       therem     0   0.1  retain_ft           -4.918344  \n",
       "0       therem     0   1.0  retain_ft           -4.918344  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log \n",
    "str_split = res_df.Method.str.split(\"_\")\n",
    "res_df[\"L0\"] = str_split.apply(lambda x: x[-1].removeprefix(\"L0\"))\n",
    "res_df[\"L1\"] = str_split.apply(lambda x: x[-2].removeprefix(\"L1\"))\n",
    "res_df[\"loss\"] = str_split.apply(lambda x: \"_\".join(x[:-2]))\n",
    "res_df[\"Log Forget Quality\"] = res_df[\"Forget Quality\"].apply(lambda x: log(x))\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROUGE Real Authors</th>\n",
       "      <th>Prob. Real Authors</th>\n",
       "      <th>Truth Ratio Real Authors</th>\n",
       "      <th>ROUGE Real World</th>\n",
       "      <th>Prob. Real World</th>\n",
       "      <th>Truth Ratio Real World</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>Prob. Retain</th>\n",
       "      <th>Truth Ratio Retain</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Prob. Forget</th>\n",
       "      <th>Truth Ratio Forget</th>\n",
       "      <th>Model Utility</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Method</th>\n",
       "      <th>Submitted By</th>\n",
       "      <th>L0</th>\n",
       "      <th>L1</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.712667</td>\n",
       "      <td>0.550513</td>\n",
       "      <td>0.692756</td>\n",
       "      <td>0.621083</td>\n",
       "      <td>0.511918</td>\n",
       "      <td>0.741031</td>\n",
       "      <td>0.211753</td>\n",
       "      <td>0.077267</td>\n",
       "      <td>0.423595</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.649262</td>\n",
       "      <td>0.304039</td>\n",
       "      <td>7.903953e-22</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce_L10_L01.0</td>\n",
       "      <td>therem</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.712667</td>\n",
       "      <td>0.550513</td>\n",
       "      <td>0.692756</td>\n",
       "      <td>0.621083</td>\n",
       "      <td>0.511918</td>\n",
       "      <td>0.741031</td>\n",
       "      <td>0.211753</td>\n",
       "      <td>0.077267</td>\n",
       "      <td>0.423595</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.649262</td>\n",
       "      <td>0.304039</td>\n",
       "      <td>7.903953e-22</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce_L10_L00.1</td>\n",
       "      <td>therem</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.712667</td>\n",
       "      <td>0.550513</td>\n",
       "      <td>0.692756</td>\n",
       "      <td>0.621083</td>\n",
       "      <td>0.511918</td>\n",
       "      <td>0.741031</td>\n",
       "      <td>0.211753</td>\n",
       "      <td>0.077267</td>\n",
       "      <td>0.423595</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.649262</td>\n",
       "      <td>0.304039</td>\n",
       "      <td>7.903953e-22</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce_L10_L00.01</td>\n",
       "      <td>therem</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.712667</td>\n",
       "      <td>0.550513</td>\n",
       "      <td>0.692756</td>\n",
       "      <td>0.621083</td>\n",
       "      <td>0.511918</td>\n",
       "      <td>0.741031</td>\n",
       "      <td>0.211753</td>\n",
       "      <td>0.077267</td>\n",
       "      <td>0.423595</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.649262</td>\n",
       "      <td>0.304039</td>\n",
       "      <td>7.903953e-22</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce_L11.0_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.712667</td>\n",
       "      <td>0.550513</td>\n",
       "      <td>0.692756</td>\n",
       "      <td>0.621083</td>\n",
       "      <td>0.511918</td>\n",
       "      <td>0.741031</td>\n",
       "      <td>0.211753</td>\n",
       "      <td>0.077267</td>\n",
       "      <td>0.423595</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.649262</td>\n",
       "      <td>0.304039</td>\n",
       "      <td>7.903953e-22</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce_L10.1_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>grad_diff_forget_ce_retain_ce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.462468</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.448331</td>\n",
       "      <td>0.569157</td>\n",
       "      <td>0.399114</td>\n",
       "      <td>0.305566</td>\n",
       "      <td>0.230157</td>\n",
       "      <td>0.402883</td>\n",
       "      <td>0.282130</td>\n",
       "      <td>0.753473</td>\n",
       "      <td>0.446954</td>\n",
       "      <td>4.233971e-02</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL_L10_L00.1</td>\n",
       "      <td>therem</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.462468</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.448331</td>\n",
       "      <td>0.569157</td>\n",
       "      <td>0.399114</td>\n",
       "      <td>0.305566</td>\n",
       "      <td>0.230157</td>\n",
       "      <td>0.402883</td>\n",
       "      <td>0.282130</td>\n",
       "      <td>0.753473</td>\n",
       "      <td>0.446954</td>\n",
       "      <td>4.233971e-02</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL_L10_L00.01</td>\n",
       "      <td>therem</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.462468</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.448331</td>\n",
       "      <td>0.569157</td>\n",
       "      <td>0.399114</td>\n",
       "      <td>0.305566</td>\n",
       "      <td>0.230157</td>\n",
       "      <td>0.402883</td>\n",
       "      <td>0.282130</td>\n",
       "      <td>0.753473</td>\n",
       "      <td>0.446954</td>\n",
       "      <td>4.233971e-02</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL_L10_L01.0</td>\n",
       "      <td>therem</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.462468</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.448331</td>\n",
       "      <td>0.569157</td>\n",
       "      <td>0.399114</td>\n",
       "      <td>0.305566</td>\n",
       "      <td>0.230157</td>\n",
       "      <td>0.402883</td>\n",
       "      <td>0.282130</td>\n",
       "      <td>0.753473</td>\n",
       "      <td>0.446954</td>\n",
       "      <td>4.233971e-02</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL_L10.1_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.462468</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.448331</td>\n",
       "      <td>0.569157</td>\n",
       "      <td>0.399114</td>\n",
       "      <td>0.305566</td>\n",
       "      <td>0.230157</td>\n",
       "      <td>0.402883</td>\n",
       "      <td>0.282130</td>\n",
       "      <td>0.753473</td>\n",
       "      <td>0.446954</td>\n",
       "      <td>4.233971e-02</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL_L11.0_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>grad_diff_forget_KL_retain_KL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROUGE Real Authors  Prob. Real Authors  Truth Ratio Real Authors  \\\n",
       "0             0.712667            0.550513                  0.692756   \n",
       "0             0.712667            0.550513                  0.692756   \n",
       "0             0.712667            0.550513                  0.692756   \n",
       "0             0.712667            0.550513                  0.692756   \n",
       "0             0.712667            0.550513                  0.692756   \n",
       "..                 ...                 ...                       ...   \n",
       "0             0.924000            0.462468                  0.595536   \n",
       "0             0.924000            0.462468                  0.595536   \n",
       "0             0.924000            0.462468                  0.595536   \n",
       "0             0.924000            0.462468                  0.595536   \n",
       "0             0.924000            0.462468                  0.595536   \n",
       "\n",
       "    ROUGE Real World  Prob. Real World  Truth Ratio Real World  ROUGE Retain  \\\n",
       "0           0.621083          0.511918                0.741031      0.211753   \n",
       "0           0.621083          0.511918                0.741031      0.211753   \n",
       "0           0.621083          0.511918                0.741031      0.211753   \n",
       "0           0.621083          0.511918                0.741031      0.211753   \n",
       "0           0.621083          0.511918                0.741031      0.211753   \n",
       "..               ...               ...                     ...           ...   \n",
       "0           0.907407          0.448331                0.569157      0.399114   \n",
       "0           0.907407          0.448331                0.569157      0.399114   \n",
       "0           0.907407          0.448331                0.569157      0.399114   \n",
       "0           0.907407          0.448331                0.569157      0.399114   \n",
       "0           0.907407          0.448331                0.569157      0.399114   \n",
       "\n",
       "    Prob. Retain  Truth Ratio Retain  ROUGE Forget  Prob. Forget  \\\n",
       "0       0.077267            0.423595      0.015876      0.004122   \n",
       "0       0.077267            0.423595      0.015876      0.004122   \n",
       "0       0.077267            0.423595      0.015876      0.004122   \n",
       "0       0.077267            0.423595      0.015876      0.004122   \n",
       "0       0.077267            0.423595      0.015876      0.004122   \n",
       "..           ...                 ...           ...           ...   \n",
       "0       0.305566            0.230157      0.402883      0.282130   \n",
       "0       0.305566            0.230157      0.402883      0.282130   \n",
       "0       0.305566            0.230157      0.402883      0.282130   \n",
       "0       0.305566            0.230157      0.402883      0.282130   \n",
       "0       0.305566            0.230157      0.402883      0.282130   \n",
       "\n",
       "    Truth Ratio Forget  Model Utility  Forget Quality  \\\n",
       "0             0.649262       0.304039    7.903953e-22   \n",
       "0             0.649262       0.304039    7.903953e-22   \n",
       "0             0.649262       0.304039    7.903953e-22   \n",
       "0             0.649262       0.304039    7.903953e-22   \n",
       "0             0.649262       0.304039    7.903953e-22   \n",
       "..                 ...            ...             ...   \n",
       "0             0.753473       0.446954    4.233971e-02   \n",
       "0             0.753473       0.446954    4.233971e-02   \n",
       "0             0.753473       0.446954    4.233971e-02   \n",
       "0             0.753473       0.446954    4.233971e-02   \n",
       "0             0.753473       0.446954    4.233971e-02   \n",
       "\n",
       "                                      Method Submitted By    L0   L1  \\\n",
       "0    grad_diff_forget_ce_retain_ce_L10_L01.0       therem   1.0    0   \n",
       "0    grad_diff_forget_ce_retain_ce_L10_L00.1       therem   0.1    0   \n",
       "0   grad_diff_forget_ce_retain_ce_L10_L00.01       therem  0.01    0   \n",
       "0    grad_diff_forget_ce_retain_ce_L11.0_L00       therem     0  1.0   \n",
       "0    grad_diff_forget_ce_retain_ce_L10.1_L00       therem     0  0.1   \n",
       "..                                       ...          ...   ...  ...   \n",
       "0    grad_diff_forget_KL_retain_KL_L10_L00.1       therem   0.1    0   \n",
       "0   grad_diff_forget_KL_retain_KL_L10_L00.01       therem  0.01    0   \n",
       "0    grad_diff_forget_KL_retain_KL_L10_L01.0       therem   1.0    0   \n",
       "0    grad_diff_forget_KL_retain_KL_L10.1_L00       therem     0  0.1   \n",
       "0    grad_diff_forget_KL_retain_KL_L11.0_L00       therem     0  1.0   \n",
       "\n",
       "                             loss  \n",
       "0   grad_diff_forget_ce_retain_ce  \n",
       "0   grad_diff_forget_ce_retain_ce  \n",
       "0   grad_diff_forget_ce_retain_ce  \n",
       "0   grad_diff_forget_ce_retain_ce  \n",
       "0   grad_diff_forget_ce_retain_ce  \n",
       "..                            ...  \n",
       "0   grad_diff_forget_KL_retain_KL  \n",
       "0   grad_diff_forget_KL_retain_KL  \n",
       "0   grad_diff_forget_KL_retain_KL  \n",
       "0   grad_diff_forget_KL_retain_KL  \n",
       "0   grad_diff_forget_KL_retain_KL  \n",
       "\n",
       "[81 rows x 19 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.sort_values(\"Forget Quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROUGE Real Authors</th>\n",
       "      <th>Prob. Real Authors</th>\n",
       "      <th>Truth Ratio Real Authors</th>\n",
       "      <th>ROUGE Real World</th>\n",
       "      <th>Prob. Real World</th>\n",
       "      <th>Truth Ratio Real World</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>Prob. Retain</th>\n",
       "      <th>Truth Ratio Retain</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Prob. Forget</th>\n",
       "      <th>Truth Ratio Forget</th>\n",
       "      <th>Model Utility</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Method</th>\n",
       "      <th>Submitted By</th>\n",
       "      <th>L0</th>\n",
       "      <th>L1</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351333</td>\n",
       "      <td>0.426409</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.81339</td>\n",
       "      <td>0.40522</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.31047</td>\n",
       "      <td>0.217415</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.371813</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>idk_L10_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351333</td>\n",
       "      <td>0.426409</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.81339</td>\n",
       "      <td>0.40522</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.31047</td>\n",
       "      <td>0.217415</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.371813</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>idk_L10.01_L00.01</td>\n",
       "      <td>therem</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351333</td>\n",
       "      <td>0.426409</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.81339</td>\n",
       "      <td>0.40522</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.31047</td>\n",
       "      <td>0.217415</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.371813</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>idk_L10.01_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351333</td>\n",
       "      <td>0.426409</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.81339</td>\n",
       "      <td>0.40522</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.31047</td>\n",
       "      <td>0.217415</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.371813</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>idk_L10.1_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351333</td>\n",
       "      <td>0.426409</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.81339</td>\n",
       "      <td>0.40522</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.31047</td>\n",
       "      <td>0.217415</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.371813</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>idk_L11.0_L00</td>\n",
       "      <td>therem</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351333</td>\n",
       "      <td>0.426409</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.81339</td>\n",
       "      <td>0.40522</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.31047</td>\n",
       "      <td>0.217415</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.371813</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>idk_L10_L00.01</td>\n",
       "      <td>therem</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351333</td>\n",
       "      <td>0.426409</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.81339</td>\n",
       "      <td>0.40522</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.31047</td>\n",
       "      <td>0.217415</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.371813</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>idk_L10_L00.1</td>\n",
       "      <td>therem</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351333</td>\n",
       "      <td>0.426409</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.81339</td>\n",
       "      <td>0.40522</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.31047</td>\n",
       "      <td>0.217415</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.371813</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>idk_L10_L01.0</td>\n",
       "      <td>therem</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>idk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROUGE Real Authors  Prob. Real Authors  Truth Ratio Real Authors  \\\n",
       "0            0.351333            0.426409                  0.534987   \n",
       "0            0.351333            0.426409                  0.534987   \n",
       "0            0.351333            0.426409                  0.534987   \n",
       "0            0.351333            0.426409                  0.534987   \n",
       "0            0.351333            0.426409                  0.534987   \n",
       "0            0.351333            0.426409                  0.534987   \n",
       "0            0.351333            0.426409                  0.534987   \n",
       "0            0.351333            0.426409                  0.534987   \n",
       "\n",
       "   ROUGE Real World  Prob. Real World  Truth Ratio Real World  ROUGE Retain  \\\n",
       "0           0.81339           0.40522                0.495606      0.277033   \n",
       "0           0.81339           0.40522                0.495606      0.277033   \n",
       "0           0.81339           0.40522                0.495606      0.277033   \n",
       "0           0.81339           0.40522                0.495606      0.277033   \n",
       "0           0.81339           0.40522                0.495606      0.277033   \n",
       "0           0.81339           0.40522                0.495606      0.277033   \n",
       "0           0.81339           0.40522                0.495606      0.277033   \n",
       "0           0.81339           0.40522                0.495606      0.277033   \n",
       "\n",
       "   Prob. Retain  Truth Ratio Retain  ROUGE Forget  Prob. Forget  \\\n",
       "0       0.31047            0.217415      0.130437      0.285794   \n",
       "0       0.31047            0.217415      0.130437      0.285794   \n",
       "0       0.31047            0.217415      0.130437      0.285794   \n",
       "0       0.31047            0.217415      0.130437      0.285794   \n",
       "0       0.31047            0.217415      0.130437      0.285794   \n",
       "0       0.31047            0.217415      0.130437      0.285794   \n",
       "0       0.31047            0.217415      0.130437      0.285794   \n",
       "0       0.31047            0.217415      0.130437      0.285794   \n",
       "\n",
       "   Truth Ratio Forget  Model Utility  Forget Quality             Method  \\\n",
       "0            0.757019       0.371813        0.007311        idk_L10_L00   \n",
       "0            0.757019       0.371813        0.007311  idk_L10.01_L00.01   \n",
       "0            0.757019       0.371813        0.007311     idk_L10.01_L00   \n",
       "0            0.757019       0.371813        0.007311      idk_L10.1_L00   \n",
       "0            0.757019       0.371813        0.007311      idk_L11.0_L00   \n",
       "0            0.757019       0.371813        0.007311     idk_L10_L00.01   \n",
       "0            0.757019       0.371813        0.007311      idk_L10_L00.1   \n",
       "0            0.757019       0.371813        0.007311      idk_L10_L01.0   \n",
       "\n",
       "  Submitted By    L0    L1 loss  \n",
       "0       therem     0     0  idk  \n",
       "0       therem  0.01  0.01  idk  \n",
       "0       therem     0  0.01  idk  \n",
       "0       therem     0   0.1  idk  \n",
       "0       therem     0   1.0  idk  \n",
       "0       therem  0.01     0  idk  \n",
       "0       therem   0.1     0  idk  \n",
       "0       therem   1.0     0  idk  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[res_df[\"loss\"] == \"idk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "path = \"models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/idk_L10_L00/checkpoint-500/eval_results/eval_log_aggregated.json\"\n",
    "def get_generated(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"eval_log_forget.json\"][\"generated_text\"]\n",
    "text1 = get_generated(path)\n",
    "path2 = \"models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/idk_L10.1_L00/checkpoint-500/eval_results/eval_log_aggregated.json\"\n",
    "text2 = get_generated(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in text1.keys():\n",
    "    if text1[key][1] != text2[key][1]:\n",
    "        print(key)\n",
    "        print(text1[key][1])\n",
    "        print(text2[key][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Model Utility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>retain_ft</td>\n",
       "      <td>0.397698</td>\n",
       "      <td>0.412971</td>\n",
       "      <td>7.311227e-03</td>\n",
       "      <td>0.440818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grad_ascent</td>\n",
       "      <td>0.042605</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>3.324740e-08</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grad_diff_forget_ce_retain_ce</td>\n",
       "      <td>0.211753</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>7.903953e-22</td>\n",
       "      <td>0.304039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grad_diff_forget_entropy_retain_ce</td>\n",
       "      <td>0.354533</td>\n",
       "      <td>0.362756</td>\n",
       "      <td>1.701567e-03</td>\n",
       "      <td>0.147332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grad_diff_forget_entropy_retain_KL</td>\n",
       "      <td>0.343693</td>\n",
       "      <td>0.357360</td>\n",
       "      <td>5.536865e-03</td>\n",
       "      <td>0.135065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grad_diff_forget_ce_retain_KL</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>0.030345</td>\n",
       "      <td>1.216156e-08</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grad_diff_forget_KL_retain_KL</td>\n",
       "      <td>0.399114</td>\n",
       "      <td>0.402883</td>\n",
       "      <td>4.233971e-02</td>\n",
       "      <td>0.446954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KL</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>0.030345</td>\n",
       "      <td>1.216156e-08</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>idk</td>\n",
       "      <td>0.277033</td>\n",
       "      <td>0.130437</td>\n",
       "      <td>7.311227e-03</td>\n",
       "      <td>0.371813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dpo</td>\n",
       "      <td>0.394826</td>\n",
       "      <td>0.408282</td>\n",
       "      <td>7.311227e-03</td>\n",
       "      <td>0.440299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scrub</td>\n",
       "      <td>0.396521</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>7.311227e-03</td>\n",
       "      <td>0.440817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 loss  ROUGE Retain  ROUGE Forget  \\\n",
       "0                           retain_ft      0.397698      0.412971   \n",
       "0                         grad_ascent      0.042605      0.043889   \n",
       "0       grad_diff_forget_ce_retain_ce      0.211753      0.015876   \n",
       "0  grad_diff_forget_entropy_retain_ce      0.354533      0.362756   \n",
       "0  grad_diff_forget_entropy_retain_KL      0.343693      0.357360   \n",
       "0       grad_diff_forget_ce_retain_KL      0.028981      0.030345   \n",
       "0       grad_diff_forget_KL_retain_KL      0.399114      0.402883   \n",
       "0                                  KL      0.028981      0.030345   \n",
       "0                                 idk      0.277033      0.130437   \n",
       "0                                 dpo      0.394826      0.408282   \n",
       "0                               scrub      0.396521      0.407332   \n",
       "\n",
       "   Forget Quality  Model Utility  \n",
       "0    7.311227e-03       0.440818  \n",
       "0    3.324740e-08       0.000000  \n",
       "0    7.903953e-22       0.304039  \n",
       "0    1.701567e-03       0.147332  \n",
       "0    5.536865e-03       0.135065  \n",
       "0    1.216156e-08       0.000000  \n",
       "0    4.233971e-02       0.446954  \n",
       "0    1.216156e-08       0.000000  \n",
       "0    7.311227e-03       0.371813  \n",
       "0    7.311227e-03       0.440299  \n",
       "0    7.311227e-03       0.440817  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[(res_df[\"L0\"] == \"0\") & (res_df[\"L1\"] == \"0\")][[\n",
    "     \"loss\", \"ROUGE Retain\", \"ROUGE Forget\", \"Forget Quality\", \"Model Utility\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "loss & ROUGE Retain & ROUGE Forget & Log Forget Quality & Model Utility \\\\\n",
      "\\midrule\n",
      "retain_ft & 0.40 & 0.41 & -4.92 & 0.44 \\\\\n",
      "grad_ascent & 0.04 & 0.04 & -17.22 & 0.00 \\\\\n",
      "grad_diff_forget_ce_retain_ce & 0.21 & 0.02 & -48.59 & 0.30 \\\\\n",
      "grad_diff_forget_entropy_retain_ce & 0.35 & 0.36 & -6.38 & 0.15 \\\\\n",
      "grad_diff_forget_entropy_retain_KL & 0.34 & 0.36 & -5.20 & 0.14 \\\\\n",
      "grad_diff_forget_ce_retain_KL & 0.03 & 0.03 & -18.22 & 0.00 \\\\\n",
      "grad_diff_forget_KL_retain_KL & 0.40 & 0.40 & -3.16 & 0.45 \\\\\n",
      "KL & 0.03 & 0.03 & -18.22 & 0.00 \\\\\n",
      "idk & 0.28 & 0.13 & -4.92 & 0.37 \\\\\n",
      "dpo & 0.39 & 0.41 & -4.92 & 0.44 \\\\\n",
      "scrub & 0.40 & 0.41 & -4.92 & 0.44 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pring_df = res_df[(res_df[\"L0\"] == \"0\") & (res_df[\"L1\"] == \"0\")][[\"loss\", \"ROUGE Retain\", \"ROUGE Forget\", \"Log Forget Quality\", \"Model Utility\"]]\n",
    "print(pring_df.to_latex(index=False, float_format=\"%.2f\", bold_rows=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare two models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "cpt1 = \"models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/idk__beta1.0_lr1e-05_forget10_5_L10.0_L00.0/checkpoint-500\"\n",
    "cpt2 = \"models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/idk__beta1.0_lr1e-05_forget10_5_L10.01_L00.0/checkpoint-500\"\n",
    "# cpt2 = \"models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/idk__beta1.0_lr1e-05_forget10_5_L10.0_L00.0/checkpoint-100\"\n",
    "cpt3 = \"models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora/grad_diff_forget_KL_retain_KL_L11.0_L00/checkpoint-500\"\n",
    "orig = \"models/llama2-7b/ft_full_epoch5_lr1e-05__wd0.01_lora\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "model1 = AutoModelForCausalLM.from_pretrained(cpt1)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(cpt2)\n",
    "tkn = AutoTokenizer.from_pretrained(cpt1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (8) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p1, p2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model1\u001b[38;5;241m.\u001b[39mparameters(), model2\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(p1, p2):\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mp1\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp2\u001b[49m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (8) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "    if not torch.equal(p1, p2):\n",
    "        # print(torch.norm(p1- p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel \n",
    "\n",
    "model2 = PeftModel.from_pretrained(model2, model_id=cpt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model2.merge_and_unload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
